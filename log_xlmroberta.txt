2025-03-15 16:08:02,490 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:08:02,511 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:08:02,532 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 18160.65 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5533/8508 [00:00<00:00, 25817.23 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26507.17 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31590.06 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:08:04,623 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:08:04,623 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:08:04,784 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160802.502845/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:54,  5.13it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 176.79it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 391.29it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 410.65it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 443.51it/s]Total test samples:  18%|█▊        | 385/2127 [00:01<00:03, 444.32it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 442.72it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 464.79it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 460.45it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 464.83it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 482.65it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 460.75it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 466.53it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 455.66it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 414.59it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 407.07it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 435.75it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 448.88it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 456.19it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 458.93it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 445.67it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 467.92it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 461.08it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 412.03it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 394.83it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:01, 438.42it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:04<00:00, 472.19it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 469.04it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 482.79it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 459.47it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 457.06it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 450.29it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:09:14,826 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:14,842 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:14,857 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19428.34 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5540/8508 [00:00<00:00, 26667.77 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27098.27 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31268.41 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:17,070 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:17,071 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:17,223 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160914.837204/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 360.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 13.06 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:19,283 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:19,298 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:19,313 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20560.67 examples/s]Counting labels by count_span_labels:  69%|██████▊   | 5837/8508 [00:00<00:00, 28462.29 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 28450.90 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32960.25 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:21,501 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:21,502 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:21,652 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160919.293515/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 360.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 13.06 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:23,717 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:23,730 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:23,745 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19037.89 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5749/8508 [00:00<00:00, 26608.03 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26369.66 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31870.95 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:25,948 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:25,949 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:26,104 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160923.725520/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 360.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:28,201 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:28,211 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:28,227 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20145.32 examples/s]Counting labels by count_span_labels:  70%|██████▉   | 5925/8508 [00:00<00:00, 28955.93 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 28873.39 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 34041.35 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:30,417 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:30,418 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:30,571 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160928.207145/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:32,680 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:32,695 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:32,710 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20086.08 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5766/8508 [00:00<00:00, 27605.80 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27709.96 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32048.18 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:34,856 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:34,857 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:35,011 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160932.690788/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:37,107 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:37,123 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:37,137 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20273.17 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5570/8508 [00:00<00:00, 27348.13 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27825.67 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31883.02 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:39,228 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:39,229 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:39,383 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160937.118388/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:41,446 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:41,461 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:41,476 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20173.27 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5561/8508 [00:00<00:00, 27235.91 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27721.46 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31742.70 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:43,598 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:43,599 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:43,749 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160941.456639/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:45,886 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:45,896 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:45,911 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20109.75 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5763/8508 [00:00<00:00, 27648.23 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27771.18 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32012.19 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:48,040 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:48,041 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:48,191 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160945.891616/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:50,232 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:50,242 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:50,257 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20265.22 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5765/8508 [00:00<00:00, 27687.30 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27729.49 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32167.90 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:52,367 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:52,368 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:52,521 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160950.237691/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:54,694 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:54,709 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:54,724 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19832.79 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5767/8508 [00:00<00:00, 27505.11 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27611.76 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31499.93 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:09:56,801 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:09:56,801 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:09:56,955 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160954.704866/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:09:59,042 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:09:59,057 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:09:59,072 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20573.11 examples/s]Counting labels by count_span_labels:  70%|██████▉   | 5927/8508 [00:00<00:00, 29205.95 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 29126.28 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 34041.35 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:10:01,119 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:10:01,120 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:10:01,277 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315160959.052662/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:10:03,369 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:10:03,384 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:10:03,399 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19856.16 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5534/8508 [00:00<00:00, 26897.48 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27395.53 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31459.72 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:10:05,478 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:10:05,478 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:10:05,632 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161003.379529/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:10:07,703 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:10:07,718 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:10:07,733 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 17621.66 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5802/8508 [00:00<00:00, 26274.96 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26631.06 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32525.60 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:10:09,809 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:10:09,809 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:10:09,964 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161007.713868/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:10:12,013 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:10:12,029 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:10:12,045 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 18209.55 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5773/8508 [00:00<00:00, 26435.02 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26697.42 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32032.19 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:10:14,171 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:10:14,172 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:10:14,324 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161012.024340/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:10:16,403 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:10:16,413 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:10:16,428 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20159.52 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5765/8508 [00:00<00:00, 27680.89 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27790.04 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32198.78 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:10:18,526 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:10:18,527 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:10:18,676 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161016.408401/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:10:20,816 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:10:20,831 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:10:20,847 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19763.75 examples/s]Counting labels by count_span_labels:  66%|██████▌   | 5578/8508 [00:00<00:00, 27087.93 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27575.98 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31341.79 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:10:22,797 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:10:22,798 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:10:22,951 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161020.827205/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1029375 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:10:25,058 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:10:25,073 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:10:25,088 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19546.46 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5554/8508 [00:00<00:00, 26818.60 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27383.84 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31885.08 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:10:27,255 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:10:27,256 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:10:27,410 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161025.068869/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:54,  5.13it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 179.74it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 390.77it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 407.29it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 436.39it/s]Total test samples:  18%|█▊        | 385/2127 [00:01<00:03, 442.31it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 437.29it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 462.77it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 463.40it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 461.40it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 486.87it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 468.87it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 477.79it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 472.49it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 431.59it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 429.94it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 460.34it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 455.43it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 457.64it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 453.84it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 456.15it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 475.79it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 465.05it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 446.84it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 390.03it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:01, 439.01it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:04<00:00, 473.18it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 460.97it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 471.71it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 470.23it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 467.86it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 455.74it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:11:37,280 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:11:37,290 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:11:37,305 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19821.39 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5548/8508 [00:00<00:00, 26954.95 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27271.17 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31754.33 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:11:39,284 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:11:39,285 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:11:39,440 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161137.285597/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
    return func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 767, in adamw
    func(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 600, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 2.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.10 GiB memory in use. Including non-PyTorch memory, this process has 9.40 GiB memory in use. Of the allocated memory 8.73 GiB is allocated by PyTorch, and 221.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:11:41,625 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:11:41,640 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:11:41,655 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19849.64 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5551/8508 [00:00<00:00, 26986.38 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27445.46 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31825.02 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:11:43,742 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:11:43,743 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:11:43,892 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161141.635962/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 968.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:11:46,062 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:11:46,078 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:11:46,093 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19882.23 examples/s]Counting labels by count_span_labels:  64%|██████▍   | 5482/8508 [00:00<00:00, 26625.88 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27303.13 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31763.15 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:11:48,193 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:11:48,194 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:11:48,345 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161146.073101/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 968.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:11:50,505 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:11:50,520 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:11:50,534 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19617.46 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5786/8508 [00:00<00:00, 27431.04 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27495.79 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31798.59 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:11:52,604 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:11:52,605 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:11:52,758 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161150.515540/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:11:54,989 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:11:55,004 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:11:55,020 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19918.83 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5559/8508 [00:00<00:00, 27076.66 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27636.25 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32041.74 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:11:57,055 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:11:57,056 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:11:57,209 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161155.000147/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:11:59,312 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:11:59,322 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:11:59,338 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20261.18 examples/s]Counting labels by count_span_labels:  67%|██████▋   | 5671/8508 [00:00<00:00, 27669.59 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 28252.95 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 33706.31 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:01,402 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:01,403 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:01,557 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161159.317851/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:03,625 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:03,640 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:03,655 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19740.80 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5564/8508 [00:00<00:00, 26995.48 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27501.03 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31932.21 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:05,734 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:05,735 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:05,885 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161203.635939/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:07,979 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:07,994 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:08,010 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19261.69 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5525/8508 [00:00<00:00, 26485.63 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26914.81 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 30947.24 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:10,190 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:10,191 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:10,348 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161207.989927/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:12,518 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:12,534 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:12,549 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20059.34 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5524/8508 [00:00<00:00, 26963.51 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27416.30 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31511.28 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:14,747 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:14,747 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:14,901 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161212.529027/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:16,963 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:16,978 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:16,993 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20963.33 examples/s]Counting labels by count_span_labels:  69%|██████▉   | 5893/8508 [00:00<00:00, 29217.89 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 29139.43 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 33986.62 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:19,158 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:19,158 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:19,314 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161216.973606/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:21,433 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:21,448 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:21,462 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19958.32 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5776/8508 [00:00<00:00, 27569.42 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27696.93 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31487.26 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:23,639 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:23,640 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:23,794 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161221.443362/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:25,902 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:25,912 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:25,927 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19992.89 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5767/8508 [00:00<00:00, 27529.71 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27618.75 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31625.89 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:28,116 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:28,117 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:28,271 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161225.907865/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:30,339 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:30,354 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:30,370 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20543.91 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5789/8508 [00:00<00:00, 28073.02 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 28140.61 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32484.74 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:32,500 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:32,500 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:32,652 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161230.349753/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:34,693 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:34,708 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:34,723 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19908.04 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5774/8508 [00:00<00:00, 27501.89 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27552.64 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31508.94 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:36,856 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:36,857 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:37,011 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161234.703992/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:39,124 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:39,139 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:39,154 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19428.38 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5544/8508 [00:00<00:00, 26694.19 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27221.51 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31541.25 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:41,253 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:41,253 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:41,413 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161239.134601/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:43,494 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:43,509 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:43,524 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20149.32 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5523/8508 [00:00<00:00, 27011.78 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27520.35 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31688.01 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:45,441 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:45,441 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:45,593 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161243.504877/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 966.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1033985 has 12.47 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:12:47,739 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:12:47,754 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:12:47,769 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20259.33 examples/s]Counting labels by count_span_labels:  69%|██████▉   | 5850/8508 [00:00<00:00, 28282.62 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 28127.68 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 22789.25 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:12:49,552 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:12:49,553 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:12:49,707 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161247.749605/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<09:37,  3.68it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:13, 154.49it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:05, 355.12it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 380.25it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 419.09it/s]Total test samples:  18%|█▊        | 385/2127 [00:01<00:04, 417.37it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 424.60it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 453.48it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 455.03it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 447.59it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:03, 470.76it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 454.92it/s]Total test samples:  39%|███▉      | 833/2127 [00:02<00:02, 464.87it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 461.67it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 424.28it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 426.31it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 459.37it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 463.49it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 468.41it/s]Total test samples:  60%|██████    | 1281/2127 [00:03<00:01, 467.46it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 475.33it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 484.09it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 440.08it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 417.37it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 399.28it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:01, 435.86it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:04<00:00, 481.93it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:04<00:00, 455.91it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 453.09it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 432.90it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 433.60it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 428.51it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 439.02it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:13:59,935 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:13:59,946 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:13:59,962 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 18436.29 examples/s]Counting labels by count_span_labels:  64%|██████▍   | 5441/8508 [00:00<00:00, 25505.29 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 25334.84 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 24538.43 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 30742.91 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:02,149 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:02,150 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:02,309 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161359.941936/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 227, in step
    adamw(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
    return func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 767, in adamw
    func(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 600, in _multi_tensor_adamw
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 192.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 11.15 GiB memory in use. Including non-PyTorch memory, this process has 10.17 GiB memory in use. Of the allocated memory 9.49 GiB is allocated by PyTorch, and 233.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:04,571 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:04,586 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:04,601 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19265.10 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5509/8508 [00:00<00:00, 26398.16 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26905.96 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31031.53 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:06,713 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:06,714 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:06,865 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161404.581979/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 159, in _init_group
    state["exp_avg_sq"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 360.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 13.06 GiB memory in use. Including non-PyTorch memory, this process has 8.09 GiB memory in use. Of the allocated memory 7.41 GiB is allocated by PyTorch, and 236.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:09,055 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:09,070 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:09,086 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19356.23 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5549/8508 [00:00<00:00, 26673.42 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27186.07 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31700.85 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:11,217 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:11,218 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:11,369 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161409.065881/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 360.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:13,596 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:13,611 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:13,626 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19360.20 examples/s]Counting labels by count_span_labels:  56%|█████▌    | 4741/8508 [00:00<00:00, 22245.98 examples/s]Counting labels by count_span_labels:  94%|█████████▍| 8000/8508 [00:00<00:00, 26475.83 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 25204.00 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31591.85 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:15,890 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:15,890 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:16,042 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161413.606758/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:18,139 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:18,155 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:18,170 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19682.20 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5754/8508 [00:00<00:00, 27277.76 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27404.74 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 22435.81 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:20,382 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:20,382 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:20,533 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161418.150284/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:22,629 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:22,645 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:22,660 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19541.83 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5561/8508 [00:00<00:00, 26855.48 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27289.60 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31319.35 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:24,878 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:24,878 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:25,034 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161422.640424/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:27,074 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:27,089 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:27,106 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19479.20 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5504/8508 [00:00<00:00, 26505.69 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27033.46 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31320.67 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:29,330 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:29,331 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:29,481 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161427.084902/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:31,648 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:31,664 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:31,678 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19842.92 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5564/8508 [00:00<00:00, 27054.34 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27474.90 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31741.00 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:33,871 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:33,872 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:34,023 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161431.659356/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:36,173 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:36,188 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:36,203 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19936.49 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5572/8508 [00:00<00:00, 27158.73 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27692.99 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26663.95 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 30505.96 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:38,285 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:38,286 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:38,438 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161436.184047/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:40,516 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:40,531 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:40,546 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19770.62 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5520/8508 [00:00<00:00, 26767.74 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27260.21 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 30735.71 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:42,658 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:42,659 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:42,812 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161440.526595/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:44,857 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:44,872 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:44,886 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20119.62 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5798/8508 [00:00<00:00, 27807.20 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27908.10 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 25494.70 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:47,051 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:47,051 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:47,206 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161444.867574/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:49,235 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:49,245 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:49,259 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19844.98 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5553/8508 [00:00<00:00, 26998.39 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27428.19 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31260.63 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:51,386 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:51,386 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:51,536 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161449.240473/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:53,582 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:53,598 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:53,613 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20052.98 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5762/8508 [00:00<00:00, 27542.08 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27628.01 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31686.89 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:14:55,705 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:14:55,705 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:14:55,862 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161453.593386/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:14:58,040 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:14:58,055 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:14:58,070 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20013.62 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5510/8508 [00:00<00:00, 26859.75 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27449.09 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31307.81 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:15:00,137 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:15:00,138 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:15:00,289 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161458.050402/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:15:02,396 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:15:02,412 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:15:02,427 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19875.03 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5563/8508 [00:00<00:00, 27071.32 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27602.32 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31664.51 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:15:04,541 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:15:04,541 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:15:04,695 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161502.407423/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:15:06,811 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:15:06,826 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:15:06,841 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19407.35 examples/s]Counting labels by count_span_labels:  66%|██████▌   | 5587/8508 [00:00<00:00, 26914.73 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27355.10 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31677.55 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:15:08,791 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:15:08,792 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:15:08,946 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161506.821999/metrics.json

Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1234, in train_loop
    self.invoke_hook(TrainerStages.after_train_iter)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 94, in after_train_iter
    self.processor.backward(trainer, self.loss_keys, self.cumulative_iters,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/optimizer/base.py", line 44, in backward
    trainer.optimizer.step()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 130, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 216, in step
    has_complex = self._init_group(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/torch/optim/adamw.py", line 155, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 978.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 358.44 MiB is free. Process 70971 has 2.12 GiB memory in use. Process 1038230 has 14.02 GiB memory in use. Including non-PyTorch memory, this process has 7.14 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 235.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2025-03-15 16:15:11,038 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:15:11,053 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:15:11,068 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19649.57 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5771/8508 [00:00<00:00, 27321.09 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27431.35 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31457.61 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:15:12,829 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:15:12,829 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:15:12,985 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161511.048643/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<07:29,  4.73it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 177.17it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 387.42it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 381.66it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 414.38it/s]Total test samples:  18%|█▊        | 385/2127 [00:01<00:04, 410.34it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:04, 417.35it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 445.78it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 450.96it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 455.87it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 482.63it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 464.32it/s]Total test samples:  39%|███▉      | 833/2127 [00:02<00:02, 476.34it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 469.43it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 410.86it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 413.82it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 448.42it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 448.98it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 459.48it/s]Total test samples:  60%|██████    | 1281/2127 [00:03<00:01, 466.23it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 478.89it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 490.42it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 479.51it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 444.59it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 410.34it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:01, 446.28it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 478.78it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:04<00:00, 448.85it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 439.71it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 450.44it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 450.12it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 443.44it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 446.67it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:16:23,125 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:16:23,140 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:16:23,155 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19697.66 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5569/8508 [00:00<00:00, 26995.51 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27488.27 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31746.88 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:16:24,858 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:16:24,858 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:16:25,014 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161623.136025/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:30,  5.44it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 186.85it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 401.59it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 414.41it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 445.88it/s]Total test samples:  18%|█▊        | 385/2127 [00:01<00:04, 434.23it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 437.62it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 464.56it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 465.46it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 469.35it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 493.06it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 466.39it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 472.90it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 464.34it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 424.06it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 425.58it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 456.84it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 464.47it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 474.61it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 474.26it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 484.75it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 496.79it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 481.97it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 457.38it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 413.59it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:01, 452.17it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 495.74it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:04<00:00, 469.92it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 472.33it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 487.98it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 480.34it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 473.19it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 463.70it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:17:31,582 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:17:31,597 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:17:31,612 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19824.52 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5541/8508 [00:00<00:00, 26920.12 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27700.29 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26617.89 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31692.85 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:17:33,439 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:17:33,440 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:17:33,593 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161731.592986/metrics.json
Terminated
2025-03-15 16:18:17,021 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:18:17,036 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:18:17,051 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 17281.77 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5787/8508 [00:00<00:00, 25903.80 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26197.06 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31902.52 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:18:18,787 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:18:18,788 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:18:18,941 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161817.031789/metrics.json
Terminated
2025-03-15 16:18:31,819 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:18:31,834 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:18:31,849 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19721.85 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5549/8508 [00:00<00:00, 26901.11 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 28024.39 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 26818.95 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 26363.45 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:18:33,611 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:18:33,612 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:18:33,765 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161831.830113/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:32,  5.42it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:10, 190.78it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 408.35it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 420.72it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 446.76it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 453.38it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 452.54it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 478.73it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 470.60it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 470.15it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 486.43it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 460.43it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 463.07it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 460.72it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 421.36it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 425.49it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 458.60it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 466.03it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 470.49it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 471.25it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 480.26it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 494.12it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 481.36it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 458.52it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 423.25it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 467.81it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 508.05it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 477.68it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 471.96it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 490.22it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 480.33it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 466.83it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 465.72it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:19:40,322 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:19:40,337 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:19:40,352 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19919.08 examples/s]Counting labels by count_span_labels:  66%|██████▌   | 5584/8508 [00:00<00:00, 27211.11 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27690.85 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31874.48 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:19:42,159 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:19:42,160 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:19:42,314 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315161940.332563/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:45,  5.24it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 185.12it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 403.33it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 420.59it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 454.09it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 454.87it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 450.74it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 473.76it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 469.23it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 471.53it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 494.12it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 471.93it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 478.01it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 470.05it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 425.69it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 425.80it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 459.56it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 467.37it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 473.98it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 474.74it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 486.53it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 495.81it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 480.83it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 453.32it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 427.68it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 471.66it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 511.40it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 476.32it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 470.81it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 485.26it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 472.78it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 466.19it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 466.69it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:20:48,879 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:20:48,894 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:20:48,910 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19307.03 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5778/8508 [00:00<00:00, 27259.57 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27378.59 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31913.82 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:20:50,622 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:20:50,623 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:20:50,774 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315162048.889624/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:40,  5.31it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:10, 190.38it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 408.02it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 420.22it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 449.97it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 455.63it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 451.87it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 476.09it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 471.32it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 471.96it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 496.01it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 472.61it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 477.98it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 472.35it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 430.39it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 430.88it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 465.63it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 470.24it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 471.29it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 470.11it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 475.79it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 489.44it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 475.78it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 451.72it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 424.33it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 465.55it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 487.96it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 483.90it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 497.69it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 486.23it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 474.00it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 468.39it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:21:57,246 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:21:57,261 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:21:57,276 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19950.56 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5522/8508 [00:00<00:00, 26886.07 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27443.63 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31595.43 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:21:58,978 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:21:58,979 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:21:59,134 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315162157.256685/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:30,  5.44it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 187.44it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 405.46it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 419.28it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 451.54it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 457.23it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 455.29it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 479.84it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 472.64it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 467.97it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 490.70it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 470.24it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 482.29it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 474.85it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 434.63it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 429.87it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 456.37it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 462.14it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 465.61it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 470.35it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 479.76it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 494.22it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 479.91it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 451.11it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 430.76it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 476.47it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 492.61it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 487.88it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 498.37it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 485.74it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 476.06it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 469.27it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:23:05,437 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:23:05,452 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:23:05,467 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20269.73 examples/s]Counting labels by count_span_labels:  65%|██████▌   | 5566/8508 [00:00<00:00, 27321.04 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27240.23 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32011.04 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:23:07,283 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:23:07,284 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:23:07,435 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315162305.447531/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:39,  5.33it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:10, 188.10it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 405.81it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 421.25it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 450.84it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 452.35it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 449.98it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 473.41it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 467.86it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 466.17it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 491.52it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 473.97it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 477.57it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 472.13it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 432.20it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 438.18it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 470.35it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 469.68it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 478.26it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 477.98it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 484.57it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 498.11it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 485.30it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 459.87it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 437.22it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 478.44it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 515.43it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 482.41it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 479.23it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 491.46it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 480.73it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 470.32it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 470.30it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:24:13,716 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:24:13,733 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:24:13,747 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19923.73 examples/s]Counting labels by count_span_labels:  66%|██████▌   | 5575/8508 [00:00<00:00, 27166.59 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27631.95 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31751.17 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:24:15,459 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:24:15,460 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:24:15,613 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315162413.726744/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:46,  5.24it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:10, 188.64it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 406.76it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 420.96it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 450.43it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 456.90it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 452.70it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 477.79it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 470.52it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 470.40it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 494.80it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 471.15it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 480.88it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 471.57it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 429.08it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 432.06it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 461.58it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 466.82it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 474.66it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 475.88it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 482.08it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 493.20it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 483.54it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 461.65it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 432.33it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 476.49it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 494.76it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 490.46it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 501.82it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 491.12it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 482.59it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 471.30it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:25:22,037 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:25:22,052 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:25:22,067 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20239.89 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5768/8508 [00:00<00:00, 27796.88 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27851.53 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31969.17 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:25:23,759 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:25:23,760 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:25:23,911 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315162522.047767/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:29,  5.46it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:09, 221.41it/s]Total test samples:   6%|▌         | 129/2127 [00:00<00:06, 317.95it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 426.41it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 455.15it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 459.25it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 456.19it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 478.17it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 475.32it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 473.16it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 493.47it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 471.57it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 478.48it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 471.90it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 433.53it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 432.16it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 465.80it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 468.12it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 475.09it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 475.75it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 483.96it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 498.71it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 484.53it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 457.83it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 428.95it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 474.30it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 492.91it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 486.77it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 499.40it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 490.11it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 478.61it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 471.56it/s]
Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:31,  5.44it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:09, 221.79it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 448.81it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 441.47it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 462.61it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 467.15it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 464.01it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 487.60it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 482.01it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 481.02it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 501.56it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 483.04it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 493.09it/s]Total test samples:  42%|████▏     | 897/2127 [00:01<00:02, 478.67it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 437.76it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 436.68it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 470.28it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 476.00it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 476.83it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 476.64it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:02<00:01, 485.66it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 501.28it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 487.32it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 455.91it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 432.68it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 477.14it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 492.67it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 484.33it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 496.31it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 485.25it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 474.67it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 478.06it/s]
Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:58,  5.07it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:09, 218.62it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 439.66it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 445.19it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 465.34it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 454.17it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 455.64it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 480.74it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 471.41it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 470.91it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 494.68it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 476.89it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 487.76it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 478.85it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 434.12it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 435.50it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 464.04it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 423.92it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:02, 446.01it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 452.20it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 467.77it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 484.84it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 479.12it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 459.24it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 432.98it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 475.18it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 514.73it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 480.11it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 474.34it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 488.67it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 484.08it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 473.24it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 470.25it/s]
Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<07:00,  5.05it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:09, 210.50it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 427.98it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 433.32it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 456.16it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 462.22it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 458.68it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 481.84it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 476.19it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 478.23it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 502.51it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 470.46it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 474.24it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 467.77it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 428.30it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 429.43it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 461.99it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 469.66it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 476.43it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 472.40it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:02<00:01, 482.59it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 495.01it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 482.01it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 457.98it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 432.69it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 475.42it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 512.77it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 478.56it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 480.78it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 494.40it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 481.44it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 469.35it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 472.10it/s]
Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:59,  5.07it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:09, 216.49it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 436.09it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 442.81it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 473.30it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 469.81it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 459.37it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 479.05it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 476.41it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 476.96it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 498.11it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 474.03it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 480.17it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 465.90it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 424.33it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 429.13it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 460.49it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 469.38it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 474.82it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 475.43it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:02<00:01, 483.95it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 494.43it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 479.89it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 460.14it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 426.96it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 470.97it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 488.01it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 479.85it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 495.23it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 485.93it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 469.87it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 472.78it/s]
Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<05:18,  6.67it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:10, 203.01it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 423.10it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 431.28it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 458.36it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 453.95it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 454.97it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 474.14it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 466.98it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 467.68it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 492.12it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 473.72it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 484.61it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 476.48it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 426.96it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 423.10it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 455.38it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 460.91it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 468.82it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 472.72it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 487.44it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 500.29it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 478.64it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 452.40it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 419.34it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 464.62it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 505.28it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 483.48it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 483.61it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 498.30it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 487.68it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 478.98it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 471.74it/s]2025-03-15 16:31:00,672 - INFO - adaseq.training.default_trainer - test: {
  "precision": 0.0,
  "recall": 0.0,
  "f1": 0.0,
  "precision-MACRO": 0.0,
  "recall-MACRO": 0.0,
  "f1-MACRO": 0.0,
  "precision-UNLABELED": 0.0,
  "recall-UNLABELED": 0.0,
  "f1-UNLABELED": 0.0,
  "precision-Act_active": 0.0,
  "recall-Act_active": 0.0,
  "f1-Act_active": 0.0,
  "precision-Act_light": 0.0,
  "recall-Act_light": 0.0,
  "f1-Act_light": 0.0,
  "precision-Act_stationary": 0.0,
  "recall-Act_stationary": 0.0,
  "f1-Act_stationary": 0.0,
  "precision-F_Sign": 0.0,
  "recall-F_Sign": 0.0,
  "f1-F_Sign": 0.0,
  "precision-F_Transport": 0.0,
  "recall-F_Transport": 0.0,
  "f1-F_Transport": 0.0,
  "precision-F_active": 0.0,
  "recall-F_active": 0.0,
  "f1-F_active": 0.0,
  "precision-F_catering": 0.0,
  "recall-F_catering": 0.0,
  "f1-F_catering": 0.0,
  "precision-F_cultural": 0.0,
  "recall-F_cultural": 0.0,
  "f1-F_cultural": 0.0,
  "precision-F_lights": 0.0,
  "recall-F_lights": 0.0,
  "f1-F_lights": 0.0,
  "precision-F_path": 0.0,
  "recall-F_path": 0.0,
  "f1-F_path": 0.0,
  "precision-F_recreation": 0.0,
  "recall-F_recreation": 0.0,
  "f1-F_recreation": 0.0,
  "precision-F_service": 0.0,
  "recall-F_service": 0.0,
  "f1-F_service": 0.0,
  "precision-F_stationary": 0.0,
  "recall-F_stationary": 0.0,
  "f1-F_stationary": 0.0,
  "precision-N_Animal": 0.0,
  "recall-N_Animal": 0.0,
  "f1-N_Animal": 0.0,
  "precision-N_Color": 0.0,
  "recall-N_Color": 0.0,
  "f1-N_Color": 0.0,
  "precision-N_Flower": 0.0,
  "recall-N_Flower": 0.0,
  "f1-N_Flower": 0.0,
  "precision-N_Plant": 0.0,
  "recall-N_Plant": 0.0,
  "f1-N_Plant": 0.0,
  "precision-N_Terrain": 0.0,
  "recall-N_Terrain": 0.0,
  "f1-N_Terrain": 0.0,
  "precision-N_Water": 0.0,
  "recall-N_Water": 0.0,
  "f1-N_Water": 0.0,
  "precision-N_Weather": 0.0,
  "recall-N_Weather": 0.0,
  "f1-N_Weather": 0.0,
  "precision-P_Crowd": 0.0,
  "recall-P_Crowd": 0.0,
  "f1-P_Crowd": 0.0,
  "precision-P_Olf": 0.0,
  "recall-P_Olf": 0.0,
  "f1-P_Olf": 0.0,
  "precision-P_Sound": 0.0,
  "recall-P_Sound": 0.0,
  "f1-P_Sound": 0.0,
  "precision-P_Tactile": 0.0,
  "recall-P_Tactile": 0.0,
  "f1-P_Tactile": 0.0
}

2025-03-15 16:31:02,348 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:31:02,364 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:31:02,378 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20033.68 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5794/8508 [00:00<00:00, 27868.90 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27926.07 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32261.19 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:31:04,210 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:31:04,210 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:31:04,368 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315163102.359322/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:35,  5.38it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 186.95it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 405.27it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 420.77it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 453.28it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 459.53it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 451.91it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 473.93it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 465.81it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 466.09it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 489.27it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 469.60it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 479.53it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 474.03it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 434.69it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 429.80it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 460.85it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 461.51it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 468.61it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 473.17it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 482.38it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 496.71it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 482.29it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 457.06it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 426.69it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 468.28it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 483.95it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 476.97it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 487.54it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 477.47it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 470.06it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 467.06it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:32:10,728 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:32:10,737 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:32:10,752 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20346.56 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5790/8508 [00:00<00:00, 27950.39 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27505.94 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32102.50 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:32:12,483 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:32:12,484 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:32:12,636 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315163210.733073/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:47,  5.21it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 186.31it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 404.04it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 420.39it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 451.81it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 457.15it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 456.20it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 480.88it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 466.08it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 465.86it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 489.58it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 468.64it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 477.77it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 472.74it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 435.20it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 433.76it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 464.97it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 471.30it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 475.06it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 472.86it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 482.78it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 497.49it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 479.88it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 453.97it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 430.99it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 472.43it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 510.96it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 485.70it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 474.40it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 487.36it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 478.37it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 471.53it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 468.78it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:33:19,224 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:33:19,240 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:33:19,255 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20194.42 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5530/8508 [00:00<00:00, 27073.74 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27622.24 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31707.27 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:33:20,945 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:33:20,946 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:33:21,098 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315163319.235236/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:33,  5.40it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:09, 218.84it/s]Total test samples:   6%|▌         | 129/2127 [00:00<00:06, 310.03it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 403.89it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 421.69it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 453.93it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 459.95it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 443.31it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 468.53it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 459.56it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 459.88it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 484.39it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 465.41it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 473.02it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 468.91it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 431.01it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 431.99it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 462.78it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 466.06it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 471.73it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 471.76it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 481.06it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 496.44it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 482.42it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 455.25it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 432.40it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 477.04it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 494.32it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 490.35it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 500.87it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 491.03it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 481.50it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 467.58it/s]Terminated
2025-03-15 16:34:30,352 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:34:30,367 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:34:30,382 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20218.26 examples/s]Counting labels by count_span_labels:  67%|██████▋   | 5714/8508 [00:00<00:00, 27389.98 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27595.88 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31717.98 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:34:32,082 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:34:32,082 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:34:32,249 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315163430.362781/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:44,  5.25it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 185.95it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 401.64it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 418.62it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 452.18it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 455.24it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 444.57it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 466.73it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 461.98it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 457.96it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 477.09it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 461.75it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 471.89it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 467.60it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 426.48it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 425.80it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 460.54it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 463.24it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 469.49it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 464.36it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 473.34it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 488.95it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 480.04it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 450.44it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 414.32it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:01, 460.89it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:04<00:00, 485.18it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 481.97it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 494.12it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 481.83it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 465.12it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 462.70it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:35:38,725 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:35:38,740 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:35:38,755 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20041.08 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5773/8508 [00:00<00:00, 27687.83 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27832.68 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32204.25 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:35:40,545 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:35:40,546 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:35:40,701 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315163538.735991/metrics.json
Terminated
2025-03-15 16:35:46,064 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:35:46,079 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:35:46,094 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19832.58 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5775/8508 [00:00<00:00, 27601.18 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27110.74 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31507.05 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:35:47,804 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:35:47,811 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:35:47,964 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315163546.074520/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:31,  5.43it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:10, 187.53it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 403.01it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 416.10it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 446.73it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 454.30it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 450.08it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 476.17it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 468.62it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 462.85it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 488.25it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 471.68it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 476.22it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 471.75it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 429.47it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 425.74it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 458.32it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 461.94it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 466.03it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 460.05it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 470.13it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 483.16it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 471.82it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 450.17it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 424.67it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 472.19it/s]Total test samples:  81%|████████▏ | 1729/2127 [00:03<00:00, 511.01it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:04<00:00, 483.60it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 480.93it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 494.79it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 480.10it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 467.01it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 465.30it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:36:54,425 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:36:54,440 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:36:54,455 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20497.24 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5789/8508 [00:00<00:00, 28040.95 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27561.94 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32278.47 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:36:56,169 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:36:56,169 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:36:56,329 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315163654.436041/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:33,  5.40it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:10, 188.27it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 403.31it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 419.86it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 452.09it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 454.09it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 450.80it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 475.79it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 464.45it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 461.09it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 486.38it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 459.72it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 468.31it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 463.45it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 426.74it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 427.42it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 459.89it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 464.29it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 465.55it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 468.91it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 480.14it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 490.61it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 478.62it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 455.20it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 429.08it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 473.52it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 488.41it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 479.86it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 490.49it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 477.59it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 467.08it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 465.08it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:38:02,874 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:38:02,889 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:38:02,904 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19494.78 examples/s]Counting labels by count_span_labels:  67%|██████▋   | 5741/8508 [00:00<00:00, 27060.32 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27207.60 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31883.48 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:38:04,719 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:38:04,720 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:38:04,872 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315163802.884372/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:42,  5.28it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:11, 186.00it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 403.94it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 418.63it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:04, 447.34it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 449.39it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 448.56it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 475.85it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 469.41it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 469.40it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 494.31it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 471.89it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 482.45it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 475.47it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 430.70it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 437.65it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 465.27it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 471.09it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 480.35it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 480.49it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 487.25it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 499.31it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 485.19it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 459.68it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 424.63it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 466.71it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 487.18it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 484.19it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 497.80it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 484.96it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 472.18it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 469.39it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:39:11,306 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:39:11,321 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:39:11,336 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 20178.04 examples/s]Counting labels by count_span_labels:  68%|██████▊   | 5786/8508 [00:00<00:00, 27794.72 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27903.74 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 32224.25 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:39:13,042 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:39:13,043 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:39:13,197 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315163911.316591/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:38,  5.34it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:10, 187.70it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 406.16it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 418.38it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 452.08it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 454.03it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 447.55it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 472.00it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 469.09it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 467.93it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 491.33it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 470.68it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 479.77it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 476.27it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 435.20it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 438.16it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 469.90it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 471.11it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 476.04it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 478.33it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 487.90it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 497.79it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 477.62it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 448.98it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 423.10it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 470.10it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 489.32it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 483.87it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 496.17it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 486.91it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 478.49it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 469.73it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
2025-03-15 16:40:19,635 - modelscope - WARNING - The reference has been Deprecated in modelscope v1.4.0+, please use `from modelscope.msdatasets.dataset_cls.custom_datasets import TorchCustomDataset`
2025-03-15 16:40:19,650 - INFO - adaseq.data.dataset_manager - Will use a custom loading script: /home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_builders/named_entity_recognition_dataset_builder.py
2025-03-15 16:40:19,665 - INFO - adaseq.data.dataset_manager - First sample in train set: {'id': '0', 'tokens': ['We', 'took', 'this', 'and', 'you', 'have', 'an', 'amazing', 'view', 'of', 'the', 'downtown', 'area', 'and', 'the', 'harbour', '.'], 'spans': [{'start': 15, 'end': 16, 'type': 'N_Water'}, {'start': 1, 'end': 2, 'type': 'Act_light'}], 'mask': [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]}
Counting labels by count_span_labels:   0%|          | 0/8508 [00:00<?, ? examples/s]Counting labels by count_span_labels:  27%|██▋       | 2278/8508 [00:00<00:00, 19761.09 examples/s]Counting labels by count_span_labels:  65%|██████▍   | 5491/8508 [00:00<00:00, 26602.93 examples/s]Counting labels by count_span_labels: 100%|██████████| 8508/8508 [00:00<00:00, 27006.84 examples/s]
Counting labels by count_span_labels:   0%|          | 0/2127 [00:00<?, ? examples/s]Counting labels by count_span_labels: 100%|██████████| 2127/2127 [00:00<00:00, 31525.98 examples/s]** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
** build_dataset error log: 'global-pointer-model is not in the custom_datasets registry group named-entity-recognition. Please make sure the correct version of ModelScope library is used.'
2025-03-15 16:40:21,382 - INFO - adaseq.training.default_trainer - device: cuda:0
2025-03-15 16:40:21,382 - INFO - adaseq.training.optimizer - Number of trainable parameters: 560072882
2025-03-15 16:40:21,536 - INFO - adaseq.training.hooks.text_logger_hook - Text logs will be saved to: /home/wp/wenpei/wenpei/AdaSeq-master/experiments/gp_modernbert_large_20250110data_grid_search/250315164019.645881/metrics.json

Total test samples:   0%|          | 0/2127 [00:00<?, ?it/s]Total test samples:   0%|          | 1/2127 [00:00<06:30,  5.44it/s]Total test samples:   3%|▎         | 65/2127 [00:00<00:10, 188.68it/s]Total test samples:   9%|▉         | 193/2127 [00:00<00:04, 407.93it/s]Total test samples:  12%|█▏        | 257/2127 [00:00<00:04, 421.18it/s]Total test samples:  15%|█▌        | 321/2127 [00:00<00:03, 452.33it/s]Total test samples:  18%|█▊        | 385/2127 [00:00<00:03, 458.00it/s]Total test samples:  21%|██        | 449/2127 [00:01<00:03, 452.52it/s]Total test samples:  24%|██▍       | 513/2127 [00:01<00:03, 475.98it/s]Total test samples:  27%|██▋       | 577/2127 [00:01<00:03, 466.67it/s]Total test samples:  30%|███       | 641/2127 [00:01<00:03, 467.90it/s]Total test samples:  33%|███▎      | 705/2127 [00:01<00:02, 488.16it/s]Total test samples:  36%|███▌      | 769/2127 [00:01<00:02, 472.00it/s]Total test samples:  39%|███▉      | 833/2127 [00:01<00:02, 482.27it/s]Total test samples:  42%|████▏     | 897/2127 [00:02<00:02, 474.86it/s]Total test samples:  45%|████▌     | 961/2127 [00:02<00:02, 435.90it/s]Total test samples:  48%|████▊     | 1025/2127 [00:02<00:02, 434.10it/s]Total test samples:  51%|█████     | 1089/2127 [00:02<00:02, 465.65it/s]Total test samples:  54%|█████▍    | 1153/2127 [00:02<00:02, 465.85it/s]Total test samples:  57%|█████▋    | 1217/2127 [00:02<00:01, 471.87it/s]Total test samples:  60%|██████    | 1281/2127 [00:02<00:01, 471.26it/s]Total test samples:  63%|██████▎   | 1345/2127 [00:03<00:01, 480.19it/s]Total test samples:  66%|██████▌   | 1409/2127 [00:03<00:01, 494.81it/s]Total test samples:  69%|██████▉   | 1473/2127 [00:03<00:01, 478.10it/s]Total test samples:  72%|███████▏  | 1537/2127 [00:03<00:01, 452.79it/s]Total test samples:  75%|███████▌  | 1601/2127 [00:03<00:01, 425.67it/s]Total test samples:  78%|███████▊  | 1665/2127 [00:03<00:00, 472.92it/s]Total test samples:  84%|████████▍ | 1793/2127 [00:03<00:00, 491.32it/s]Total test samples:  87%|████████▋ | 1857/2127 [00:04<00:00, 485.38it/s]Total test samples:  90%|█████████ | 1921/2127 [00:04<00:00, 495.75it/s]Total test samples:  93%|█████████▎| 1985/2127 [00:04<00:00, 485.28it/s]Total test samples:  96%|█████████▋| 2049/2127 [00:04<00:00, 475.32it/s]Total test samples: 100%|██████████| 2127/2127 [00:04<00:00, 469.48it/s]
Traceback (most recent call last):
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/scripts/train.py", line 41, in <module>
    train_model_from_args(args)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 84, in train_model_from_args
    train_model(
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/commands/train.py", line 164, in train_model
    trainer.train(checkpoint_path)
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/training/default_trainer.py", line 146, in train
    return super().train(checkpoint_path=checkpoint_path, *args, **kwargs)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 711, in train
    self.train_loop(self.train_dataloader)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1243, in train_loop
    self.invoke_hook(TrainerStages.after_train_epoch)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1395, in invoke_hook
    getattr(hook, fn_name)(self)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 54, in after_train_epoch
    self.do_evaluate(trainer)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/hooks/evaluation_hook.py", line 67, in do_evaluate
    eval_res = trainer.evaluate()
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 800, in evaluate
    metric_values = self.evaluation_loop(self.eval_dataloader,
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/trainer.py", line 1295, in evaluation_loop
    metric_values = single_gpu_test(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 77, in single_gpu_test
    return get_metric_values(metric_classes)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/site-packages/modelscope/trainers/utils/inference.py", line 194, in get_metric_values
    metric_values.update(metric_cls.evaluate())
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/base.py", line 26, in evaluate
    self.dump()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 38, in dump
    self._dump_to_jsonline()
  File "/home/wp/wenpei/wenpei/AdaSeq-master/adaseq/data/dataset_dumpers/named_entity_recognition_dataset_dumper.py", line 80, in _dump_to_jsonline
    file.write(json.dumps(example, ensure_ascii=False) + '\n')
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/__init__.py", line 234, in dumps
    return cls(
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/wp/anaconda3/envs/promptner/lib/python3.8/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type int64 is not JSON serializable
